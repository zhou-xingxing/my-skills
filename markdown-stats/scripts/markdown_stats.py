#!/usr/bin/env python3
"""
Markdown æ–‡ä»¶ç»Ÿè®¡è„šæœ¬
è¯»å–æŒ‡å®šè·¯å¾„çš„ markdown æ–‡ä»¶ï¼Œè¾“å‡ºå­—ç¬¦æ•°å’Œé¢„ä¼°é˜…è¯»æ—¶é—´
"""

import sys
import re
import argparse
from pathlib import Path


# é˜…è¯»ç±»å‹é…ç½®
READING_TYPES = {
    "tech": {
        "name": "æŠ€æœ¯æ–‡æ¡£",
        "wpm": 200,
        "description": "é€‚åˆéœ€è¦æ·±å…¥ç†è§£çš„æŠ€æœ¯ç±»æ–‡ç« "
    },
    "normal": {
        "name": "æ™®é€šé˜…è¯»",
        "wpm": 350,
        "description": "é€‚åˆä¸€èˆ¬æ€§çš„æ–‡ç« é˜…è¯»"
    },
    "skim": {
        "name": "å¿«é€Ÿæµè§ˆ",
        "wpm": 550,
        "description": "é€‚åˆå¿«é€Ÿäº†è§£å¤§æ„"
    }
}


def extract_text(content: str) -> str:
    """æå–çº¯æ–‡æœ¬ï¼ˆå»é™¤ markdown æ ‡è®°ï¼‰"""
    # ç§»é™¤ä»£ç å—æ ‡è®°ï¼ˆ```languageï¼‰ï¼Œä½†ä¿ç•™å†…å®¹
    text = re.sub(r'^```[\w]*\n', '', content, flags=re.MULTILINE)
    text = re.sub(r'\n?```\s*$', '', text, flags=re.MULTILINE)
    # ç§»é™¤è¡Œå†…ä»£ç æ ‡è®°ï¼ˆ`ï¼‰ï¼Œä½†ä¿ç•™å†…å®¹
    text = re.sub(r'`([^`]*)`', r'\1', text)

    # ç§»é™¤å›¾ç‰‡
    text = re.sub(r'!\[([^\]]*)\]\([^)]+\)', '', text)
    # ç§»é™¤é“¾æ¥ï¼Œä¿ç•™é“¾æ¥æ–‡å­—
    text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
    # ç§»é™¤ HTML æ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # ç§»é™¤ markdown æ ‡é¢˜ã€åˆ—è¡¨ç­‰æ ‡è®°
    text = re.sub(r'^[#\-\*\+>]+\s*', '', text, flags=re.MULTILINE)
    # ç§»é™¤ç²—ä½“ã€æ–œä½“æ ‡è®°
    text = re.sub(r'\*\*?|__?', '', text)

    return text


def count_cjk_chars(text: str) -> int:
    """
    ç»Ÿè®¡ CJK (ä¸­æ—¥éŸ©) ç»Ÿä¸€è¡¨æ„æ–‡å­—æ•°é‡
    èŒƒå›´: \u4e00-\u9fff (åŸºæœ¬æ±‰å­—), \u3400-\u4dbf (æ‰©å±•A), ç­‰
    """
    cjk_pattern = re.compile(
        r'['
        r'\u4e00-\u9fff'
        r'\u3400-\u4dbf'
        r'\U00020000-\U0002a6df'
        r'\U0002a700-\U0002b73f'
        r'\U0002b740-\U0002b81f'
        r']',
        re.UNICODE
    )
    return len(cjk_pattern.findall(text))


def count_english_words(text: str) -> int:
    """
    ç»Ÿè®¡è‹±æ–‡å•è¯æ•°é‡
    è§„åˆ™ï¼šè¿ç»­çš„ [a-zA-Z] å­—ç¬¦ç®—ä¸€ä¸ªå•è¯
    """
    word_pattern = re.compile(r'[a-zA-Z]+')
    return len(word_pattern.findall(text))


def count_reading_units(content: str) -> dict:
    """
    æŒ‰é˜…è¯»å•ä½ç»Ÿè®¡ï¼š
    - 1 ä¸ªä¸­æ–‡æ±‰å­— = 1 å•ä½
    - 1 ä¸ªè‹±æ–‡å•è¯ = 1 å•ä½
    """
    # æå–çº¯æ–‡æœ¬ï¼ˆå»é™¤ markdown æ ‡è®°ï¼‰
    text = extract_text(content)

    # ä¸­æ–‡å­—æ•°ï¼ˆæ¯ä¸ªæ±‰å­—ç®— 1 å•ä½ï¼‰
    chinese = count_cjk_chars(text)

    # è‹±æ–‡å•è¯æ•°ï¼ˆæ¯ä¸ªå•è¯ç®— 1 å•ä½ï¼‰
    english_words = count_english_words(text)

    # æ€»é˜…è¯»å•ä½
    total = chinese + english_words

    return {
        "chinese": chinese,           # ä¸­æ–‡å­—æ•°
        "english_words": english_words,  # è‹±æ–‡å•è¯æ•°
        "total": total,               # æ€»é˜…è¯»å•ä½
    }


def estimate_reading_time(total_units: int, wpm: int = 200) -> dict:
    """
    é¢„ä¼°é˜…è¯»æ—¶é—´
    :param total_units: æ€»é˜…è¯»å•ä½ï¼ˆä¸­æ–‡å­—æ•° + è‹±æ–‡å•è¯æ•°ï¼‰
    :param wpm: æ¯åˆ†é’Ÿé˜…è¯»å•ä½æ•°ï¼ˆé»˜è®¤ 200 ä¸ª/åˆ†é’Ÿï¼Œé€‚åˆæŠ€æœ¯æ–‡æ¡£ï¼‰
    :return: åŒ…å«åˆ†é’Ÿå’Œç§’çš„å­—å…¸

    é˜…è¯»é€Ÿåº¦å‚è€ƒï¼š
    - å¿«é€Ÿæµè§ˆ: 500-600 ä¸ª/åˆ†é’Ÿ
    - æ™®é€šé˜…è¯»: 300-400 ä¸ª/åˆ†é’Ÿ
    - æŠ€æœ¯æ–‡æ¡£: 150-200 ä¸ª/åˆ†é’Ÿ
    """
    minutes = total_units / wpm
    total_seconds = int(minutes * 60)

    return {
        "minutes": total_seconds // 60,
        "seconds": total_seconds % 60,
        "total_seconds": total_seconds,
    }


def analyze_markdown(file_path: str, reading_type: str = "tech") -> dict:
    """åˆ†æ markdown æ–‡ä»¶

    Args:
        file_path: Markdown æ–‡ä»¶è·¯å¾„
        reading_type: é˜…è¯»ç±»å‹ï¼Œå¯é€‰ tech/normal/skim
    """
    path = Path(file_path)

    if not path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    if not path.is_file():
        raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}")

    content = path.read_text(encoding='utf-8')

    # åŸå§‹å­—ç¬¦æ•°
    raw_chars = len(content)
    # é˜…è¯»å•ä½ç»Ÿè®¡ï¼ˆä¸­æ–‡å­—æ•° + è‹±æ–‡å•è¯æ•°ï¼‰
    reading_units = count_reading_units(content)
    # è¡Œæ•°
    lines = content.count('\n') + 1

    # è·å–é˜…è¯»é€Ÿåº¦
    wpm = READING_TYPES.get(reading_type, READING_TYPES["tech"])["wpm"]

    # é¢„ä¼°é˜…è¯»æ—¶é—´
    reading_time = estimate_reading_time(reading_units["total"], wpm)

    return {
        "file_path": str(path.absolute()),
        "file_name": path.name,
        "raw_chars": raw_chars,
        "reading_units": reading_units,
        "lines": lines,
        "reading_time": reading_time,
        "reading_type": reading_type,
        "wpm": wpm
    }


def format_output(stats: dict) -> str:
    """æ ¼å¼åŒ–è¾“å‡º"""
    units = stats['reading_units']
    rt = stats['reading_time']
    reading_type_name = READING_TYPES.get(stats['reading_type'], READING_TYPES['tech'])['name']

    lines = [
        f"ğŸ“„ æ–‡ä»¶: {stats['file_name']}",
        f"ğŸ“ è·¯å¾„: {stats['file_path']}",
        "",
        f"ğŸ“– é˜…è¯»ç±»å‹: {reading_type_name}ï¼ˆ{stats['wpm']} å­—/åˆ†é’Ÿï¼‰",
        "",
        "ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:",
        f"  â€¢ æ€»è¡Œæ•°: {stats['lines']:,} è¡Œ",
        f"  â€¢ åŸå§‹å­—ç¬¦: {stats['raw_chars']:,} ä¸ª",
        "",
        "ğŸ“ é˜…è¯»å•ä½ç»Ÿè®¡ï¼ˆ1 æ±‰å­— = 1 è‹±æ–‡å•è¯ = 1 å•ä½ï¼‰:",
        f"  â€¢ ä¸­æ–‡å­—æ•°: {units['chinese']:,} å­—",
        f"  â€¢ è‹±æ–‡å•è¯: {units['english_words']:,} ä¸ª",
        f"  â€¢ æ€»é˜…è¯»å•ä½: {units['total']:,} ä¸ª",
        "",
        f"â±ï¸ é¢„ä¼°é˜…è¯»æ—¶é—´ï¼ˆ{reading_type_name}é€Ÿåº¦ï¼‰:",
        f"  â€¢ {rt['minutes']} åˆ† {rt['seconds']} ç§’",
        f"  â€¢ çº¦ {rt['total_seconds'] // 60 + (1 if rt['seconds'] > 30 else 0)} åˆ†é’Ÿ",
    ]
    return "\n".join(lines)


def format_reading_time(stats: dict) -> str:
    """æ ¼å¼åŒ–é˜…è¯»æ—¶é—´ä¸ºç®€æ´å­—ç¬¦ä¸²"""
    rt = stats['reading_time']
    if rt['total_seconds'] < 60:
        return "< 1 åˆ†é’Ÿ"
    elif rt['seconds'] > 30:
        return f"çº¦ {rt['minutes'] + 1} åˆ†é’Ÿ"
    else:
        return f"çº¦ {rt['minutes']} åˆ†é’Ÿ"


def insert_stats_banner(content: str, stats: dict) -> str:
    """
    åœ¨æ–‡ä»¶å¤´éƒ¨æ’å…¥é˜…è¯»ç»Ÿè®¡ä¿¡æ¯æç¤ºæ¡†

    æ ¼å¼:
    > [!TIP]
    >
    > *<small>æœ¬æ–‡çº¦ X å­—ï¼Œé¢„ä¼°é˜…è¯»æ—¶é—´ Y åˆ†é’Ÿã€‚</small>*

    Args:
        content: åŸå§‹æ–‡ä»¶å†…å®¹
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸

    Returns:
        æ›´æ–°åçš„å†…å®¹
    """
    units = stats['reading_units']
    reading_time_str = format_reading_time(stats)

    # æ„å»ºæç¤ºæ¡†
    banner = f"> [!TIP]\n>\n> *<small>æœ¬æ–‡çº¦ {units['total']} å­—ï¼Œé¢„ä¼°é˜…è¯»æ—¶é—´ {reading_time_str}ã€‚</small>*\n\n"

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç»Ÿè®¡æç¤ºæ¡†ï¼ˆå¯èƒ½åœ¨æ–‡ä»¶å¼€å¤´æˆ– frontmatter ä¹‹åï¼‰
    existing_banner_pattern = r'> \[!TIP\]\n>\n> \*<small>.*?</small>\*\n\n'
    if re.search(existing_banner_pattern, content):
        # æ›¿æ¢ç°æœ‰çš„æç¤ºæ¡†
        new_content = re.sub(existing_banner_pattern, banner, content, count=1)
        return new_content

    # æ£€æŸ¥æ˜¯å¦æœ‰ frontmatter
    frontmatter_match = re.match(r'^(---\s*\n.*?---\s*\n)', content, re.DOTALL)
    if frontmatter_match:
        # åœ¨ frontmatter åæ’å…¥æç¤ºæ¡†
        end_pos = frontmatter_match.end()
        new_content = content[:end_pos] + '\n' + banner + content[end_pos:]
        return new_content

    # åœ¨æ–‡ä»¶å¼€å¤´æ’å…¥æç¤ºæ¡†
    return banner + content


def insert_stats_to_file(file_path: str, reading_type: str = "tech", dry_run: bool = False) -> dict:
    """
    åˆ†æ markdown æ–‡ä»¶å¹¶å°†ç»Ÿè®¡ä¿¡æ¯æ’å…¥åˆ°æ–‡ä»¶å¤´éƒ¨

    Args:
        file_path: Markdown æ–‡ä»¶è·¯å¾„
        reading_type: é˜…è¯»ç±»å‹ï¼Œå¯é€‰ tech/normal/skim
        dry_run: å¦‚æœä¸º Trueï¼Œåˆ™ä¸å®é™…ä¿®æ”¹æ–‡ä»¶ï¼Œåªè¿”å›ç»Ÿè®¡ä¿¡æ¯

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    path = Path(file_path)

    if not path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    content = path.read_text(encoding='utf-8')
    stats = analyze_markdown(file_path, reading_type)

    if not dry_run:
        updated_content = insert_stats_banner(content, stats)
        path.write_text(updated_content, encoding='utf-8')

    return stats


def main():
    # æ„å»ºé˜…è¯»ç±»å‹å¸®åŠ©æ–‡æœ¬
    type_help = "é˜…è¯»ç±»å‹ï¼Œå¯é€‰:"
    for key, value in READING_TYPES.items():
        type_help += f"\n    {key} - {value['name']}({value['wpm']}å­—/åˆ†é’Ÿ) - {value['description']}"

    parser = argparse.ArgumentParser(
        description="Markdown æ–‡ä»¶ç»Ÿè®¡å·¥å…· - ç»Ÿè®¡å­—ç¬¦æ•°å’Œé¢„ä¼°é˜…è¯»æ—¶é—´",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
ç¤ºä¾‹:
  python scripts/markdown_stats.py article.md
  python scripts/markdown_stats.py article.md --type normal
  python scripts/markdown_stats.py article.md --insert --type skim

é˜…è¯»ç±»å‹è¯´æ˜:
  tech   - æŠ€æœ¯æ–‡æ¡£(200å­—/åˆ†é’Ÿ) - é€‚åˆéœ€è¦æ·±å…¥ç†è§£çš„æŠ€æœ¯ç±»æ–‡ç« 
  normal - æ™®é€šé˜…è¯»(350å­—/åˆ†é’Ÿ) - é€‚åˆä¸€èˆ¬æ€§çš„æ–‡ç« é˜…è¯»
  skim   - å¿«é€Ÿæµè§ˆ(550å­—/åˆ†é’Ÿ) - é€‚åˆå¿«é€Ÿäº†è§£å¤§æ„
        """
    )

    parser.add_argument("file", help="Markdown æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--insert", "-i",
        action="store_true",
        help="å°†ç»Ÿè®¡ä¿¡æ¯ä»¥æç¤ºæ¡†æ ¼å¼æ’å…¥åˆ°æ–‡ä»¶å¤´éƒ¨"
    )
    parser.add_argument(
        "--type", "-t",
        choices=list(READING_TYPES.keys()),
        default="tech",
        help="é˜…è¯»ç±»å‹ (é»˜è®¤: tech)"
    )

    args = parser.parse_args()

    try:
        if args.insert:
            stats = insert_stats_to_file(args.file, args.type, dry_run=False)
            print(format_output(stats))
            print("\nâœ… å·²æ›´æ–°æ–‡ä»¶ï¼Œæ·»åŠ äº†é˜…è¯»ç»Ÿè®¡æç¤ºæ¡†")
        else:
            stats = analyze_markdown(args.file, args.type)
            print(format_output(stats))
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)
    except ValueError as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
